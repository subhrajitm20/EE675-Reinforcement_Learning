{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A\n",
    "\n",
    "**Name**: Subhrajit Mishra                              </br>\n",
    "**Roll No.**: 201006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Stochastic Maze problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticMazeEnv(gym.Env):\n",
    "    '''\n",
    "    StochasticMazeEnv represents the Gym Environment for the Stochastic Maze environment\n",
    "    States : [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    Actions : [\"Left\":0, \"Up\":1, \"Right\":2, \"Down\":3]\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=12,no_actions=4):\n",
    "        '''\n",
    "        Constructor for the StochasticMazeEnv class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 12\n",
    "            no_actions : The no. of possible actions which is 4\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.actions_dict = {\"L\":0, \"U\":1, \"R\":2, \"D\":3}\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                2: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.8, 0, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                1: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "                2: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 1, -0.01, False)],\n",
    "                3: [(0.8, 1, -0.01, False), (0.1, 0, -0.01, False), (0.1, 2, -0.01, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.8, 1, -0.01, False), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "                2: [(0.8, 3, +1, True), (0.1, 2, -0.01, False), (0.1, 6, -0.01, False)],\n",
    "                3: [(0.8, 6, -0.01, False), (0.1, 1, -0.01, False), (0.1, 3, +1, True)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                1: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                2: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "                3: [(0.8, 3, 0, True), (0.1, 3, 0, True), (0.1, 3, 0, True)],\n",
    "            },\n",
    "            4: {\n",
    "                0: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                1: [(0.8, 0, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                2: [(0.8, 4, -0.01, False), (0.1, 0, -0.01, False), (0.1, 8, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 4, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "            },\n",
    "            6: {\n",
    "                0: [(0.8, 6, -0.01, False), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 2, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "                2: [(0.8, 7, -1, True), (0.1, 2, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 6, -0.01, False), (0.1, 7, -1, True)],\n",
    "            },\n",
    "            7: {\n",
    "                0: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                1: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                2: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "                3: [(0.8, 7, 0, True), (0.1, 7, 0, True), (0.1, 7, 0, True)],\n",
    "            },\n",
    "            8: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                1: [(0.8, 4, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                2: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 4, -0.01, False)],\n",
    "                3: [(0.8, 8, -0.01, False), (0.1, 8, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "            },\n",
    "            9: {\n",
    "                0: [(0.8, 8, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                1: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                2: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 9, -0.01, False)],\n",
    "                3: [(0.8, 9, -0.01, False), (0.1, 8, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            },\n",
    "            10: {\n",
    "                0: [(0.8, 9, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                1: [(0.8, 6, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 6, -0.01, False), (0.1, 10, -0.01, False)],\n",
    "                3: [(0.8, 10, -0.01, False), (0.1, 9, -0.01, False), (0.1, 11, -0.01, False)]\n",
    "            },\n",
    "            11: {\n",
    "                0: [(0.8, 10, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                1: [(0.8, 7, -1, True), (0.1, 10, -0.01, False), (0.1, 11, -0.01, False)],\n",
    "                2: [(0.8, 11, -0.01, False), (0.1, 7, -1, True), (0.1, 11, -0.01, False)],\n",
    "                3: [(0.8, 11, -0.01, False), (0.1, 11, -0.01, False), (0.1, 10, -0.01, False)]\n",
    "            }\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        index = np.random.choice(3,1,p=[0.8,0.1,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StochasticMazeEnv()\n",
    "env.reset()\n",
    "num_states = env.nS\n",
    "num_actions = env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases for checking the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   2 \t   2 \t -0.01 \t   False\n",
      "   0 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   1 \t   1 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   1 \t   8 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   0 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   1 \t   1 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   3 \t   1 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   2 \t   1 \t -0.01 \t   False\n",
      "   3 \t   0 \t -0.01 \t   False\n",
      "   1 \t   0 \t -0.01 \t   False\n",
      "   3 \t   4 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   0 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   1 \t   4 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   3 \t   8 \t -0.01 \t   False\n",
      "   0 \t   8 \t -0.01 \t   False\n",
      "   3 \t   9 \t -0.01 \t   False\n",
      "   1 \t   9 \t -0.01 \t   False\n",
      "   2 \t   9 \t -0.01 \t   False\n",
      "   1 \t   10 \t -0.01 \t   False\n",
      "   2 \t   11 \t -0.01 \t   False\n",
      "   1 \t   7 \t -1 \t   True\n",
      "Total Number of steps to Reach Terminal: 90\n",
      "Final Reward: -1.8900000000000006\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    rand_action = np.random.choice(4,1)[0]  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(rand_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", rand_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The random policy takes large number of steps to reach some terminal state which should be much higher than the number of the steps taken by a all 'Right' policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Right Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\t State\t Reward\t is_Terminal\n",
      "   2 \t   0 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   4 \t -0.01 \t   False\n",
      "   2 \t   8 \t -0.01 \t   False\n",
      "   2 \t   9 \t -0.01 \t   False\n",
      "   2 \t   9 \t -0.01 \t   False\n",
      "   2 \t   10 \t -0.01 \t   False\n",
      "   2 \t   11 \t -0.01 \t   False\n",
      "   2 \t   7 \t -1 \t   True\n",
      "Total Number of steps to Reach Terminal: 10\n",
      "Final Reward: -1.09\n"
     ]
    }
   ],
   "source": [
    "is_Terminal = False\n",
    "env.reset()\n",
    "count = 0\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Action\\t\" , \"State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "\n",
    "while not is_Terminal:\n",
    "    count += 1\n",
    "\n",
    "    right_action = 2  #0 -> LEFT, 1 -> UP, 2 -> RIGHT, 3 -> DOWN\n",
    "    state, reward, is_Terminal = env.step(right_action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\"  \", right_action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    \n",
    "print(\"Total Number of steps to Reach Terminal:\", count)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The right policy most of the time reaches the goal state in just 3 steps which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Find an optimal policy to navigate the given environment using Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.9, theta=1e-6, c=0):\n",
    "    V = np.zeros(num_states) \n",
    "    while True:\n",
    "        c+=1\n",
    "        for s in range(num_states):\n",
    "            if s==5: continue\n",
    "            temp = V[s]\n",
    "            er = 0\n",
    "            for a in range(num_actions):\n",
    "                transitions = env.prob_dynamics[s][a]\n",
    "                for prob, s_prime, reward, na in transitions:\n",
    "                    er += policy[s][a] * prob * (reward + gamma * V[s_prime])          \n",
    "\n",
    "            V[s] = er\n",
    "        if np.abs(temp - V[s]) < theta: break\n",
    "    print(c)\n",
    "    return V\n",
    "    \n",
    "def policy_improvement(env, V, gamma=0.9):\n",
    "    new_pol = np.zeros((num_states, num_actions))\n",
    "    for s in range(num_states):\n",
    "        if (s==5 or s==3): continue     # Stop if you reached Goal, And you will never be standing on the wall state\n",
    "        action_vals = np.zeros(num_actions)     # action_vals are basically the Q(s,a) values\n",
    "        for a in range(num_actions):\n",
    "            transitions = env.prob_dynamics[s][a]\n",
    "            for prob, next_state, reward, done in transitions: action_vals[a] += prob * (reward + gamma * V[next_state])\n",
    "        update = np.argmax(action_vals)\n",
    "        new_pol[s][update] = 1\n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration(env):\n",
    "    pol = np.ones((num_states, num_actions)) / num_actions\n",
    "    count_iter = 0\n",
    "\n",
    "    while True:\n",
    "        count_iter += 1\n",
    "        curr_pol = pol\n",
    "\n",
    "        v = policy_evaluation(env, curr_pol)\n",
    "        pol = policy_improvement(env, v)\n",
    "\n",
    "        if np.array_equal(curr_pol, pol): break\n",
    "\n",
    "    print(count_iter)\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "17\n",
      "15\n",
      "12\n",
      "4\n",
      "optimal policy:\n",
      "  p0 p1 p2 p3\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Probability of taking LEFT = p0, UP = p1, RIGHT = p2, and DOWN = p3\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = policy_iteration(env)\n",
    "print(\"optimal policy:\")\n",
    "print(\"  p0 p1 p2 p3\")\n",
    "print(optimal_policy)\n",
    "print('Probability of taking LEFT = p0, UP = p1, RIGHT = p2, and DOWN = p3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Find an optimal policy to navigate the given environment using Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "def policy_evaluation2(env, policy, V_old, gamma=0.9):\n",
    "    V = V_old\n",
    "    \n",
    "    for s in range(num_states):\n",
    "        if (s==5): continue\n",
    "        er = 0\n",
    "        for a in range(num_actions):\n",
    "            transitions = env.prob_dynamics[s][a]\n",
    "            for prob, next_state, reward, na in transitions:\n",
    "                er += policy[s][a] * prob * (reward + gamma * V_old[next_state])        # Note we are using V_old here instead of the current V\n",
    "        V[s] = er\n",
    "        \n",
    "    return V\n",
    "\n",
    "def policy_improvement2(env, V, gamma=0.9):\n",
    "    new_pol = np.zeros((num_states, num_actions))\n",
    "    for s in range(num_states):\n",
    "        if (s==5 or s==3): continue     # Stop if you reached Goal, And you will never be standing on the wall state\n",
    "        action_vals = np.zeros(num_actions)     # action_vals are basically the Q(s,a) values\n",
    "        for a in range(num_actions):\n",
    "            transitions = env.prob_dynamics[s][a]\n",
    "            for prob, next_state, reward, done in transitions: action_vals[a] += prob * (reward + gamma * V[next_state])\n",
    "        update = np.argmax(action_vals)\n",
    "        new_pol[s][update] = 1\n",
    "    return new_pol\n",
    "\n",
    "def value_iteration(env, gamma = 0.9):\n",
    "    V_old = np.zeros(num_states)\n",
    "\n",
    "    pol = np.ones((num_states, num_actions)) / num_actions\n",
    "    count_iter = 0\n",
    "\n",
    "    while True:\n",
    "        count_iter += 1\n",
    "        curr_pol = pol\n",
    "\n",
    "        v = policy_evaluation2(env, curr_pol, V_old)\n",
    "        pol = policy_improvement2(env, v)\n",
    "\n",
    "        if np.array_equal(curr_pol, pol): break\n",
    "\n",
    "    print(count_iter)\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "optimal policy:\n",
      "  p0 p1 p2 p3\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Probability of taking LEFT = p0, UP = p1, RIGHT = p2, and DOWN = p3\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = value_iteration(env)\n",
    "print(\"optimal policy:\")\n",
    "print(\"  p0 p1 p2 p3\")\n",
    "print(optimal_policy)\n",
    "print('Probability of taking LEFT = p0, UP = p1, RIGHT = p2, and DOWN = p3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Compare PI and VI in terms of convergence. Is the policy obtained by both same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "The policy obtained is the same in case of both PI and VI. </br>\n",
    "The number of steps required to converge is also same for both the cases, which is 4, in the case where the initial policy is such that all actions have equal probability in each state (0.25). But as can be seen the additional \"While\" loop in Policy Evaluation of policy iteration algorithm runs a large number of times, which is totally absent in case of value iteration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cafede8658e71bdc4b7180bcd658951c639327337cbd78715b7c29dc66075fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
